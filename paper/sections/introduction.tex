% ===== ยง1  Introduction =====
\section{Introduction}\label{sec:intro}

Large language models generate natural-language explanations for their outputs, yet mounting evidence indicates that these self-explanations are frequently unfaithful to the model's internal reasoning process.
Recent studies demonstrate that LLM rationales can be inconsistent with the computations that actually produce the answer~\cite{ye2024selfexplanation}, and that chain-of-thought traces are often post-hoc rationalizations rather than faithful accounts of inference~\cite{lanham2023measuring}.
The gap between \emph{apparent} and \emph{actual} reasoning makes the verification and maintenance of explanations a central knowledge representation challenge, particularly in domains such as medical diagnosis and legal reasoning where explanation correctness is critical.

Current approaches to improving LLM explanations fall short along two complementary dimensions.
Self-correction methods such as Self-Refine~\cite{madaan2023selfrefine} and Reflexion~\cite{shinn2023reflexion} iteratively rewrite explanations using the model's own feedback, but they operate without formal guarantees: edits are unconstrained, previously valid reasoning steps may be silently discarded, and there is no mechanism to ensure that changes are minimal or semantically justified.
On the other end of the spectrum, argumentation-based approaches such as ArgLLMs~\cite{freedman2025arglm} and ARGORA~\cite{argora2026} structure explanations into argument graphs and verify them against formal semantics, providing rigorous one-shot verdicts.
However, these frameworks treat verification as a static, terminal operation.
When new evidence arrives---a factual correction, a credible counterargument, or an updated knowledge source---they offer no principled way to update the explanation, forcing the system to either regenerate from scratch or apply ad-hoc edits that may violate the very consistency guarantees the formalism was designed to provide.
To the best of our knowledge, no existing framework provides a formal notion of \emph{minimal change} for maintaining LLM explanations under evolving evidence.

The following example, revisited throughout the paper, illustrates the problem concretely.

\begin{example}[Medical Diagnosis]\label{ex:running}
A question-answering system is asked to diagnose a patient with fatigue and joint pain.
The LLM answers ``Lupus'' with four argument units:
$a_1$~(``chronic fatigue reported''),
$a_2$~(``polyarthralgia present''),
$a_3$~(``Lupus commonly presents with these symptoms''),
and target $a_4$~(``most likely diagnosis is Lupus'').
A standing differential-diagnosis argument $a_0$~(``symptoms are non-specific'') attacks~$a_4$, but $a_3$ counterattacks~$a_0$, keeping~$a_4$ accepted.
A new lab result $a_5$~(``ANA test is negative'') attacks~$a_3$, removing the defense of~$a_4$: the differential~$a_0$ reinstates, rendering~$a_4$ no longer accepted under grounded semantics.
An unconstrained self-correction system might regenerate the entire explanation, discarding the valid units $a_1$ and~$a_2$.
A minimal-change repair instead seeks the smallest edit---such as introducing $a_6$~(``anti-dsDNA positive'') attacking~$a_5$---to restore~$a_4$ at cost~$2$ (visualized in Figure~\ref{fig:af-evolution}).
\end{example}

We propose \textsc{Argus}, a framework that bridges this gap by unifying argumentation-based verification with minimal-change repair.
Given an LLM-generated explanation, \textsc{Argus} decomposes it into atomic argument units, constructs an argumentation framework in the sense of Dung~\cite{dung1995acceptability}, and verifies whether the target claim is accepted under a chosen semantics.
When new evidence renders the explanation inconsistent, \textsc{Argus} computes a minimum-cost sequence of edit operations---adding or removing arguments and attacks---that restores the desired acceptability status.
The repair operator draws on two classical KR traditions: the AGM theory of belief revision~\cite{alchourron1985agm}, which supplies the minimal-change principle, and argumentation dynamics~\cite{cayrol2019argumentation}, which provides the formal machinery for reasoning about changes to attack structures.

Our contributions are as follows:
\begin{enumerate}
    \item \textbf{(C1)} A framework that structures LLM self-explanations as Dung-style argumentation frameworks and verifies them under grounded and preferred semantics, producing defense-set certificates that make acceptance verdicts interpretable (\S\ref{sec:method}).
    \item \textbf{(C2)} A minimal-change repair operator for explanation maintenance under evolving evidence, satisfying adapted AGM revision postulates with a complexity analysis placing the problem in P under grounded semantics and NP-complete under preferred semantics (\S\ref{sec:repair}--\S\ref{sec:theory}).
    \item \textbf{(C3)} A scalable ASP encoding with a $k$-neighborhood approximation that restricts the search space to arguments near the target, reducing solver grounding substantially while preserving repair quality (\S\ref{sec:method}).
    \item \textbf{(C4)} An empirical evaluation on HotpotQA and FEVER validating the formal properties and demonstrating improvements in faithfulness, contestability, and repair cost w.r.t.\ seven baselines (\S\ref{sec:experiments}).
\end{enumerate}

The remainder of this paper is organized as follows. \S\ref{sec:related} surveys related work; \S\ref{sec:preliminaries} introduces the formal background on argumentation frameworks and belief revision; \S\ref{sec:method} presents the \textsc{Argus} pipeline; \S\ref{sec:theory} establishes theoretical properties of the repair operator; and \S\ref{sec:experiments} reports experimental results. Example~\ref{ex:running} is revisited throughout to build intuition.

