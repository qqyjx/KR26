% ===== ยง1  Introduction =====
\section{Introduction}\label{sec:intro}

Large language models generate natural-language explanations for their outputs, yet mounting evidence indicates that these self-explanations are frequently unfaithful to the model's internal reasoning process.
Recent studies demonstrate that LLM rationales can be inconsistent with the computations that actually produce the answer~\citep{ye2024selfexplanation}, and that chain-of-thought traces are often post-hoc rationalizations rather than faithful accounts of inference~\citep{lanham2023measuring}.
The gap between \emph{apparent} and \emph{actual} reasoning makes the verification and maintenance of explanations under uncertain and evolving evidence a central challenge, particularly in domains such as medical diagnosis and legal reasoning where explanation correctness is critical.

\begin{figure}[t]
\centering
\includegraphics[width=\columnwidth]{figures/figure1-positioning.pdf}
\caption{Qualitative positioning of \textsc{Argus}. Self-correction methods (orange) repair without formal guarantees; argumentation-based methods (green) verify without principled repair. \textsc{Argus} (blue region) bridges both dimensions.}
\label{fig:positioning}
\end{figure}

As illustrated in Figure~\ref{fig:positioning}, current approaches fall short along two complementary dimensions.
Self-correction methods~\citep{madaan2023selfrefine,shinn2023reflexion,gao2023rarr} iteratively rewrite explanations but without formal guarantees---edits are unconstrained and previously valid reasoning may be silently discarded; indeed, recent work shows that LLMs cannot self-correct reasoning without external feedback~\citep{huang2024selfcorrect}.
Argumentation-based approaches~\citep{freedman2025arglm,argora2026} verify explanations against formal semantics but treat verification as terminal: when new evidence arrives, they offer no principled way to update the explanation while preserving consistency.
No existing framework provides a formal notion of \emph{minimal change} for maintaining LLM explanations under evolving evidence.

The following example, revisited throughout the paper, illustrates the problem concretely.

\begin{example}[Medical Diagnosis]\label{ex:running}
A question-answering system is asked to diagnose a patient with fatigue and joint pain.
The LLM answers ``Lupus'' with four argument units:
$a_1$~(``chronic fatigue reported''),
$a_2$~(``polyarthralgia present''),
$a_3$~(``Lupus commonly presents with these symptoms''),
and target $a_4$~(``most likely diagnosis is Lupus'').
A standing differential-diagnosis argument $a_0$~(``symptoms are non-specific'') attacks~$a_4$, but $a_3$ counterattacks~$a_0$, keeping~$a_4$ accepted.
A new lab result $a_5$~(``ANA test is negative'') attacks~$a_3$, removing the defense of~$a_4$: the differential~$a_0$ reinstates, rendering~$a_4$ no longer accepted under grounded semantics.
An unconstrained self-correction system might regenerate the entire explanation, discarding the valid units $a_1$ and~$a_2$.
A minimal-change repair instead seeks the smallest edit---such as introducing $a_6$~(``anti-dsDNA positive'') attacking~$a_5$---to restore~$a_4$ at cost~$2$ (visualized in Figure~\ref{fig:af-evolution}).
\end{example}

\begin{figure*}[t]
\centering
\subcaptionbox{$F_0$: Initial framework ($a_4$ accepted)\label{fig:af-f0}}[0.24\textwidth]{%
\begin{tikzpicture}
  \node[acc node] (a1) at (0, 2.0) {$a_1$};
  \node[acc node] (a2) at (1.2, 2.0) {$a_2$};
  \node[acc node] (a3) at (0, 1.0) {$a_3$};
  \node[acc node, tgt node] (a4) at (1.2, 1.0) {$a_4$};
  \node[rej node] (a0) at (0.6, 0) {$a_0$};
  \draw[att edge] (a3) -- (a0);
  \draw[att edge] (a0) -- (a4);
\end{tikzpicture}%
}\hfill
\subcaptionbox{$F_1$: After evidence update ($a_4$ rejected)\label{fig:af-f1}}[0.36\textwidth]{%
\begin{tikzpicture}
  \node[acc node] (a1) at (0, 2.0) {$a_1$};
  \node[acc node] (a2) at (1.2, 2.0) {$a_2$};
  \node[new node] (a5) at (-1.1, 2.5) {$a_5$};
  \node[rej node] (a3) at (0, 1.0) {$a_3$};
  \node[rej node, tgt node] (a4) at (1.2, 1.0) {$a_4$};
  \node[acc node] (a0) at (0.6, 0) {$a_0$};
  \draw[att edge] (a3) -- (a0);
  \draw[att edge] (a0) -- (a4);
  \draw[att edge] (a5) -- (a3);
\end{tikzpicture}%
}\hfill
\subcaptionbox{$F_2$: After repair ($a_4$ restored)\label{fig:af-f2}}[0.38\textwidth]{%
\begin{tikzpicture}
  \node[acc node] (a1) at (0, 2.0) {$a_1$};
  \node[acc node] (a2) at (1.2, 2.0) {$a_2$};
  \node[rej node] (a5) at (-1.1, 2.5) {$a_5$};
  \node[new node] (a6) at (-2.2, 2.5) {$a_6$};
  \node[acc node] (a3) at (0, 1.0) {$a_3$};
  \node[acc node, tgt node] (a4) at (1.2, 1.0) {$a_4$};
  \node[rej node] (a0) at (0.6, 0) {$a_0$};
  \draw[att edge] (a3) -- (a0);
  \draw[att edge] (a0) -- (a4);
  \draw[att edge] (a5) -- (a3);
  \draw[att edge, NewBlue!80!black, line width=1.1pt] (a6) -- (a5);
\end{tikzpicture}%
}
\caption{Evolution of the argumentation framework from Example~\ref{ex:running}. Green fill = accepted, red fill = rejected, blue dashed border = newly introduced, double border = target argument~$a_4$. In~(a), $a_3$ defeats the differential~$a_0$, keeping~$a_4$ accepted. In~(b), $a_5$ defeats~$a_3$, reinstating~$a_0$ and rejecting~$a_4$. The repair in~(c) adds $a_6$ attacking~$a_5$ to restore~$a_4$.}
\label{fig:af-evolution}
\end{figure*}

We propose \textsc{Argus}, a framework that bridges this gap by unifying argumentation-based verification with minimal-change repair.
Given an LLM-generated explanation, \textsc{Argus} decomposes it into atomic argument units, constructs an argumentation framework in the sense of \citet{dung1995acceptability}, and verifies whether the target claim is accepted under a chosen semantics.
When new evidence renders the explanation inconsistent, \textsc{Argus} computes a minimum-cost set of edit operations---adding or removing arguments and attacks---that restores the desired acceptability status.
The repair operator draws on two classical KR traditions: the AGM theory of belief revision~\citep{alchourron1985agm}, which supplies the minimal-change principle, and argumentation dynamics~\citep{cayrol2020argumentation}, which provides formal machinery for structural change.
Because the repair operates on an explicit graph structure external to the LLM, it admits formal guarantees---AGM compliance, complexity bounds, provable preservation of unaffected reasoning---that are unattainable when editing model internals or regenerating from scratch.

Our contributions are as follows:
\begin{enumerate}
    \item \textbf{(C1)} A framework that structures LLM self-explanations as Dung-style argumentation frameworks, verifies them under grounded and preferred semantics, and produces defense-set certificates for interpretable verdicts (\S\ref{sec:method}).
    \item \textbf{(C2)} A minimal-change repair operator satisfying adapted AGM postulates and bidirectionally characterized by them (Representation Theorem), with a complexity trichotomy: P under grounded, NP-complete under preferred/stable, and $\Sigma_2^P$-complete under skeptical stable semantics (\S\ref{sec:repair}--\S\ref{sec:theory}).
    \item \textbf{(C3)} A scalable ASP encoding with a $k$-neighborhood approximation that preserves repair quality at 99.7\% coverage (k=3) while reducing solver grounding to a tractable subproblem (\S\ref{sec:method}).
    \item \textbf{(C4)} An empirical evaluation on HotpotQA and FEVER validating the formal properties and demonstrating improvements in faithfulness, contestability, and repair cost w.r.t.\ ten baselines (\S\ref{sec:experiments}).
\end{enumerate}
