% ===== ยง6  Related Work =====
\section{Related Work}\label{sec:related}

Our work connects three lines of research: argumentation-based approaches to LLM reasoning, self-correction methods, and formal theories of belief change.

\textbf{Argumentation and LLMs.}
\citet{vassiliades2021argumentation} survey argumentation for explainable AI.
ArgLLMs~\citep{freedman2025arglm} constructs Dung-style graphs from LLM claims but treats verification as terminal; ARGORA~\citep{argora2026} orchestrates multi-agent argumentation but corrects through re-deliberation rather than formal repair; MQArgEng~\citep{mqargeng2024} improves LLM reasoning with modular engines but does not address explanation maintenance.
\textsc{Argus} differs by providing a minimal-change repair operator with AGM-compliant guarantees.
\citet{bengel2025sequence} introduce sequence explanations tracing why arguments are accepted; \textsc{Argus} addresses the dual question of how to restore acceptance.
We adopt Dung-style argumentation rather than ASPIC$^+$~\citep{modgil2014aspic} because the complexity bounds we exploit are established for this setting.

\textbf{Self-Correction and Revision.}
Self-Refine~\citep{madaan2023selfrefine} and Reflexion~\citep{shinn2023reflexion} iteratively rewrite LLM outputs without formal minimality guarantees; \citet{huang2024selfcorrect} show that LLMs cannot self-correct without external feedback.
RARR~\citep{gao2023rarr} targets surface-level attribution; SelfCheckGPT~\citep{manakul2023selfcheckgpt} detects hallucinations without repair; Chain-of-Verification~\citep{dhuliawala2024cove} and CRITIC~\citep{gou2024critic} improve accuracy but lack preservation guarantees.
\citet{matton2025walk} measure faithfulness through counterfactual interventions; our evaluation applies similar probes to argumentation-structured explanations.
\textsc{Argus} formalizes the repair search space, bounds change cost, and guarantees preservation of unaffected reasoning.

\textbf{Belief Revision and Argumentation Dynamics.}
The AGM theory~\citep{alchourron1985agm} and the revision/update distinction~\citep{katsuno1992update} provide classical foundations.
\citet{hase2024fundamental} argue that model editing is fundamentally a belief revision problem; our work sidesteps neural-level challenges by operating on an external argumentation structure.
In argumentation, \citet{cayrol2020argumentation} and \citet{baumann2010complexity} study how structural modifications affect extensions; \citet{costemarquis2014enforcement}, \citet{wallner2017complexity}, and \citet{bisquert2013repair} formalize argumentation revision as minimal structural change.
\citet{mailly2024constrained} extends enforcement to constrained incomplete frameworks; \citet{alfano2024counterfactual} develop counterfactual explanations via ASP, identifying minimal changes that reverse verdicts, whereas \textsc{Argus} restores verdicts disrupted by external evidence.
\textsc{Argus} extends these foundations to LLM explanation maintenance with a weighted cost model tailored to argument confidence.

