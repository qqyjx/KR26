% ===== ยง6  Related Work =====
\section{Related Work}\label{sec:related}

Our work connects three lines of research: argumentation-based approaches to LLM reasoning, self-correction methods for language models, and formal theories of belief change in argumentation.

\textbf{Argumentation and LLMs.}
Vassiliades et al.~\cite{vassiliades2021argumentation} survey the growing intersection of argumentation and explainable AI, identifying formal argumentation as a principled substrate for structuring, verifying, and contesting model explanations; our work instantiates this vision for LLM self-explanations with a concrete repair operator.
Several recent proposals structure LLM outputs using argumentation frameworks.
ArgLLMs~\cite{freedman2025arglm} decomposes LLM-generated claims into Dung-style argument graphs and applies grounded and preferred semantics to determine acceptability, producing explainable and contestable verification verdicts.
However, ArgLLMs treats verification as a one-shot, terminal operation: once the argument graph is constructed and evaluated, there is no mechanism to update the explanation when new evidence arrives, forcing the user to regenerate the entire rationale from scratch.
ARGORA~\cite{argora2026} orchestrates multiple LLM agents through an argumentation-mediated dialogue, incorporating causal semantics to ground the reasoning process.
While ARGORA includes a correction mechanism, it operates through agent re-deliberation rather than through a formally defined repair operator with cost minimization and provable guarantees.
MQArgEng~\cite{mqargeng2024} investigates whether modular argumentation engines can improve LLM reasoning accuracy, demonstrating gains on structured tasks but without addressing explanation maintenance under evolving evidence.
\textsc{Argus} differs from all three in providing a minimal-change repair operator with AGM-compliant guarantees and complexity-theoretic characterization, treating explanation maintenance as a first-class optimization problem rather than an afterthought.
Complementarily, Bengel and Thimm~\cite{bengel2025sequence} introduce \emph{sequence explanations} that trace why an argument is accepted through a serialization of the defense chain; \textsc{Argus} addresses the dual question of \emph{how to restore} acceptance when it is lost, and the two approaches could be composed: sequence explanations could make repair certificates interpretable.
We adopt Dung-style abstract argumentation rather than structured frameworks such as ASPIC$^+$~\cite{modgil2014aspic} or bipolar argumentation because the repair operator requires only the attack relation to define enforcement problems, and the complexity bounds we exploit (Theorem~\ref{thm:complexity}) are established for this setting; extending the approach to frameworks with explicit support is a natural direction for future work.

\textbf{Self-Correction and Revision.}
Self-Refine~\cite{madaan2023selfrefine} iteratively rewrites LLM outputs using the model's own feedback, while Reflexion~\cite{shinn2023reflexion} augments this paradigm with episodic memory to guide subsequent attempts.
Both methods can improve output quality, but their edits are unconstrained: there is no formal notion of minimality, previously correct reasoning steps may be silently discarded, and the revision process offers no semantic guarantees about which parts of the explanation remain intact.
Huang et al.~\cite{huang2024selfcorrect} further demonstrate that LLMs cannot self-correct reasoning without external feedback, underscoring the need for a principled external verification and repair mechanism.
RARR~\cite{gao2023rarr} retrieves external evidence to revise LLM statements, yet its edits target surface-level attribution without modeling the inferential structure that connects claims.
SelfCheckGPT~\cite{manakul2023selfcheckgpt} detects hallucinations through sampling consistency but provides no native repair mechanism.
Chain-of-Verification~\cite{dhuliawala2024cove} plans and executes verification questions to reduce hallucination, and CRITIC~\cite{gou2024critic} enables self-correction through tool-interactive critiquing; both improve factual accuracy but, like Self-Refine, lack formal guarantees on what is preserved across revisions.
In contrast, \textsc{Argus} formalizes the repair search space as edits to an argumentation framework, bounds the cost of change, and guarantees that unaffected reasoning steps are preserved.

\textbf{Belief Revision and Argumentation Dynamics.}
The AGM theory~\cite{alchourron1985agm} and the revision/update distinction~\cite{katsuno1992update} provide the classical foundations for principled belief change.
Hase et al.~\cite{hase2024fundamental} argue that model editing in LLMs is fundamentally a belief revision problem and identify challenges in applying AGM rationality criteria to neural knowledge stores; our work sidesteps these challenges by operating on an \emph{external} argumentation structure rather than on model parameters, making the AGM postulates directly applicable.
Our evidence update~$\Delta$ is closer to the Katsuno--Mendelzon notion of \emph{update} (adapting beliefs to a changed world) than to \emph{revision} (incorporating new information about a static world), since each~$\Delta$ reflects genuinely new evidence rather than a correction of prior beliefs; however, we adopt the AGM postulates as rationality criteria because the minimal-change desiderata they formalize are independent of this distinction.
In argumentation, Cayrol et al.~\cite{cayrol2019argumentation} and Baumann and Brewka~\cite{baumann2010complexity} study how structural modifications affect extensions and the complexity of enforcement; Coste-Marquis et al.~\cite{costemarquis2014enforcement}, Wallner et al.~\cite{wallner2017complexity}, and Bisquert et al.~\cite{bisquert2013repair} formalize argumentation revision as minimal status or structural change.
Alfano et al.~\cite{alfano2024counterfactual} develop counterfactual explanations for abstract argumentation via weak-constrained ASP encodings; their approach identifies minimal changes that would \emph{reverse} an acceptance verdict, whereas \textsc{Argus} computes minimal changes that \emph{restore} a verdict disrupted by external evidence.
In particular, Coste-Marquis et al.\ enforce a desired extension through minimum structural modifications, whereas our formulation targets a single argument's status, incorporates evidence updates as a first-class input, and supports heterogeneous cost functions that reflect argument-level confidence.
Our repair operator instantiates these ideas for LLM explanation maintenance, introducing a weighted cost model tailored to argument confidence and structural role.

