% ===== ยง5  Experiments =====
\section{Experimental Evaluation}\label{sec:experiments}

We evaluate \textsc{Argus} on two established benchmarks to answer three questions:
(Q1)~Do the formal properties from \S\ref{sec:theory} hold in practice?
(Q2)~Does the minimal-change repair operator improve faithfulness and contestability w.r.t.\ existing baselines?
(Q3)~What is the empirical cost of repair?

We sample 500 instances from HotpotQA~\citep{yang2018hotpotqa}, a multi-hop question-answering benchmark, and 500 from FEVER~\citep{thorne2018fever}, a fact-verification benchmark; instances are drawn with seed 42.
For each instance, we withhold one gold supporting fact during explanation generation and reintroduce it as an evidence update~$\Delta$, producing adversarial updates that target the reasoning chain.
GPT-4o~\citep{openai2023gpt4} (\texttt{gpt-4o-2024-11-20}) generates initial explanations at temperature~0.2; relation discovery uses DeBERTa-v3-large fine-tuned on MultiNLI with threshold~0.7; repairs are computed by clingo~5.6 with $k{=}3$ under uniform cost.
Results are averaged over 5 runs (std~$\leq$0.02 for accuracy, $\leq$0.4 for cost); FLARE and FactScore use a single deterministic run.  Further details appear in Appendix~\ref{app:exp-details}.

We evaluate six metrics (detailed definitions in Appendix~\ref{app:exp-details}): \emph{faithfulness} (fraction of units whose removal changes the answer), \emph{contestability} (fraction of gold counterarguments integrated), \emph{repair accuracy}, \emph{repair cost} (Definition~\ref{def:repair}), \emph{coherence} (BERTScore~\citep{zhang2020bertscore}), and \emph{solve time}.

We compare against ten baselines in three categories: self-correction methods---SelfCheckGPT~\citep{manakul2023selfcheckgpt}, Self-Refine~\citep{madaan2023selfrefine}, Reflexion~\citep{shinn2023reflexion}, RARR~\citep{gao2023rarr}; verification-oriented methods---CoT-Verifier~\citep{ling2023deductive}, ArgLLMs~\citep{freedman2025arglm}, FLARE~\citep{jiang2023flare}, FactScore~\citep{min2023factscore}; and argumentation-based methods---ARGORA~\citep{argora2026} and a na\"{i}ve \emph{Regenerate} baseline. Verification-only methods lack repair and are marked N/A. Baseline cost measures count regenerated units across up to 3 rounds, which are not directly commensurable with structural graph edits (Appendix~\ref{app:exp-details}).

Table~\ref{tab:main} summarizes the main results. \textsc{Argus} achieves the highest faithfulness (\resultFaithHotpot{}/\resultFaithFEVER{}) and contestability (\resultContestHotpot{}/\resultContestFEVER{}), with relative improvements of \improveFaithfulness{} and \improveContestability{} over ARGORA; all 12 pairwise differences are significant at $p < 0.001$ (Bonferroni-corrected $z$-tests, Cohen's $h \in [0.26, 0.38]$). Among repair-capable methods, \textsc{Argus} requires the fewest operations---\resultRepairCostHotpot{} vs.\ 5.1 for ARGORA---validating the minimal-change objective. The na\"{i}ve Regenerate baseline achieves the fastest solve time (0.5\,s) but its coherence (.65/.63)---the lowest among repair methods---confirms that complete regeneration disrupts consistency more than targeted structural repair.

\textsc{Argus} also achieves the highest coherence (.82/.80) and an average solve time of 0.55\,s/0.47\,s, which is 5--10$\times$ faster than self-correction methods. The formal properties from \S\ref{sec:theory} are confirmed empirically: success and inclusion hold by construction; vacuity holds without exception. Scalability experiments on synthetic frameworks confirm polynomial scaling for grounded semantics and the effectiveness of the $k$-neighborhood approximation, keeping preferred repair tractable up to $|\mathcal{A}|{=}50$ (Figure~\ref{fig:scalability}, Appendix~\ref{app:ablation}). Ablation results (Table~\ref{tab:ablation}, Appendix~\ref{app:ablation}) confirm that semantic verification is the most critical component ($-$5.4pp faithfulness when removed), while 83\% of repairs require at most 4~operations. A pilot human evaluation (Appendix~\ref{app:human-eval}) corroborates these results: annotators preferred \textsc{Argus} in 68\% of comparisons vs.\ Self-Refine in 19\% ($\kappa{=}0.62$, $r{=}0.78$).
