% ===== ยง7  Conclusion =====
\section{Conclusion}\label{sec:conclusion}

We presented \textsc{Argus}, a framework that structures LLM self-explanations as argumentation frameworks, verifies them against formal semantics, and repairs them at minimum cost when new evidence arrives.
The minimal-change repair operator satisfies adapted AGM postulates---success, inclusion, and vacuity---providing formal guarantees that are absent from existing self-correction and argumentation-based approaches.
We established that the repair problem is tractable under grounded semantics and NP-complete under preferred and stable semantics, and introduced a $k$-neighborhood approximation that maintains scalability in practice.
Experiments on HotpotQA and FEVER yielded relative improvements of up to \improveFaithfulness{} in faithfulness and \improveContestability{} in contestability over the strongest argumentation baseline, while achieving the lowest repair cost among all repair-capable methods.

Several limitations warrant discussion.
First, the quality of the argumentation framework depends on the LLM's ability to decompose rationales into atomic argument units; extraction errors propagate through the entire pipeline.
Second, while the $k$-neighborhood approximation handles the framework sizes encountered in our experiments, frameworks with hundreds of densely connected arguments may require more aggressive approximation strategies.
Third, our evaluation relies on automatic metrics over fact-checking and multi-hop QA datasets, with evidence updates constructed by withholding gold supporting facts; a human evaluation of explanation quality and naturalistically occurring evidence updates would further strengthen the empirical validation, and extending the approach to open-ended generation where the notion of a ``correct'' explanation is less well-defined remains an open challenge.
Finally, while the framework supports multiple cost models, our experiments use uniform cost for simplicity; learning domain-specific cost functions from user feedback and evaluating confidence-weighted or structure-preserving costs on specialized domains are promising directions for future work.
Relatedly, the confidence-weighted cost model relies on the LLM's self-assessed confidence scores, whose calibration varies across models and domains; integrating external calibration signals could improve cost model fidelity.

