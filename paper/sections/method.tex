% ===== ยง3  ARGUS Method =====
\section{The \textsc{Argus} Framework}\label{sec:method}

\begin{figure*}[t]
\centering
\begin{tikzpicture}[
  stage/.style={rectangle, rounded corners=3pt, draw=black, fill=blue!8,
                minimum height=0.85cm, minimum width=1.6cm, text width=1.4cm,
                align=center, font=\small\bfseries},
  io/.style={rectangle, rounded corners=2pt, draw=gray, fill=gray!10,
             minimum height=0.55cm, font=\small, align=center},
  detail/.style={font=\scriptsize\itshape, text=gray!60!black, align=center},
  arrow/.style={-{Stealth[length=1.5mm]}, semithick, draw=black!70},
  node distance=0.08cm and 0.4cm
]
  \node[io] (input) {$(q,a,e)$};
  \node[stage, right=0.5cm of input] (s1) {Extract};
  \node[detail, below=0.04cm of s1] (d1) {LLM$\to$JSON};
  \node[stage, right=of s1] (s2) {Relation\\Disc.};
  \node[detail, below=0.04cm of s2] (d2) {NLI+Tmpl};
  \node[stage, right=of s2] (s3) {Verify};
  \node[detail, below=0.04cm of s3] (d3) {$\sigma(F)$};
  \node[stage, right=of s3, fill=green!12, draw=green!50!black] (s4) {Repair};
  \node[detail, below=0.04cm of s4] (d4) {ASP+$k$};
  \node[io, right=0.5cm of s4] (output) {$(G,v,\rho)$};
  \draw[arrow] (input) -- (s1);
  \draw[arrow] (s1) -- (s2);
  \draw[arrow] (s2) -- (s3);
  \draw[arrow] (s3) -- (s4);
  \draw[arrow] (s4) -- (output);
  \node[io, above=0.35cm of s4] (delta) {$\Delta$};
  \draw[arrow, dashed] (delta) -- (s4);
\end{tikzpicture}
\caption{The \textsc{Argus} pipeline. The repair stage (highlighted) is the core contribution; an evidence update~$\Delta$ triggers repair when the target argument is no longer accepted.}
\label{fig:pipeline}
\end{figure*}

We now present \textsc{Argus}, a four-stage pipeline (Figure~\ref{fig:pipeline}) that transforms an unverifiable LLM rationale into a formally grounded, repairable explanation.
Given a question~$q$, an answer~$a$, and a free-form rationale~$e$, the pipeline proceeds through structured extraction (\S\ref{sec:extraction}), relation discovery (\S\ref{sec:relation}), semantic verification (\S\ref{sec:verification}), and minimal-change repair (\S\ref{sec:repair}).
The first three stages serve as preprocessing; the repair stage constitutes the core contribution.

\subsection{Structured Extraction}\label{sec:extraction}

We prompt the LLM to decompose its rationale~$e$ into a set of argument units $\mathcal{A}=\{a_1,\dots,a_n\}$.
Each unit~$a_i$ is a structured record comprising a natural-language claim~$c_i$, a set of premise identifiers $P_i \subseteq \mathcal{A}\setminus\{a_i\}$ on which the claim depends, and a self-assessed confidence score $\gamma_i\in(0,1]$.
The prompt constrains the LLM to produce a JSON array of objects, each with fields \texttt{claim}, \texttt{premises}, and \texttt{confidence}, ensuring that every claim is atomic---that is, it asserts exactly one proposition that can be independently verified or rebutted.
We designate one distinguished unit~$a_t\in\mathcal{A}$ as the \emph{target argument}, whose claim directly supports the answer~$a$.

\subsection{Relation Discovery and Graph Construction}\label{sec:relation}

Given the argument units~$\mathcal{A}$, we construct an argumentation framework $\mathit{AF}=(\mathcal{A},\mathcal{R})$ as defined in Definition~\ref{def:af}.
For every ordered pair $(a_i,a_j)$ with $i\neq j$, we query a natural language inference (NLI) model to classify the relationship between~$c_i$ and~$c_j$.
A \emph{contradiction} verdict yields an attack $(a_i,a_j)\in\mathcal{R}$, while an \emph{entailment} verdict records a support link used for downstream analysis but not encoded in~$\mathcal{R}$, since Dung-style frameworks model attacks only~\cite{dung1995acceptability}.
To improve recall on domain-specific rebuttals, we maintain an \emph{attack template library}---a curated set of negation patterns, common exceptions, and defeasible-rule conflicts.
Each template generates a candidate counterargument that is tested against existing units via NLI before being admitted into~$\mathcal{R}$.

\subsection{Semantic Verification}\label{sec:verification}

With the framework $\mathit{AF}=(\mathcal{A},\mathcal{R})$ in hand, we compute its extensions under a chosen semantics~$\sigma$ such as grounded or preferred semantics.
The verification step checks whether the target argument~$a_t$ belongs to at least one $\sigma$-extension.
If $a_t$ is \emph{accepted}, the explanation is deemed internally consistent; if $a_t$ is \emph{rejected} or \emph{undecided}, the framework flags a verification failure.
In either case, the solver also returns a \emph{defense set}~$D\subseteq\mathcal{A}$---the minimal subset of arguments whose collective acceptability entails the status of~$a_t$---which serves as a compact certificate explaining the verdict to the user.

\subsection{Minimal-Change Repair}\label{sec:repair}

When new evidence contradicts the current explanation or the verification step detects a failure, \textsc{Argus} repairs the argumentation framework rather than regenerating the rationale from scratch.
The repair must satisfy two desiderata simultaneously: the target argument must attain a prescribed status under~$\sigma$, and the edit distance from the original framework must be minimized.
We formalize this requirement below.

\textbf{Repair Operations.}
We define four elementary edit operations: $\mathsf{add\_arg}(a)$ and $\mathsf{del\_arg}(a)$ insert or remove an argument (deletions cascade to incident attacks), while $\mathsf{add\_att}(a_i,a_j)$ and $\mathsf{del\_att}(a_i,a_j)$ insert or remove attacks.
A sequence of operations yields a repaired framework $\mathit{AF}'=(\mathcal{A}',\mathcal{R}')$.

\textbf{Cost Function.}
Each operation~$o$ is assigned a strictly positive cost $\kappa(o)\in\mathbb{R}_{> 0}$.
We consider three cost models.
Under \emph{uniform cost}, every operation costs~$1$, so the objective reduces to minimizing the total number of edits.
Under \emph{confidence-weighted cost}, argument deletions are weighted by the confidence of the removed argument, $\kappa(\mathsf{del\_arg}(a_i))=\gamma_i$ (recall $\gamma_i > 0$ for all extracted arguments), while additions retain unit cost $\kappa(\mathsf{add\_arg})=\kappa(\mathsf{add\_att})=1$, reflecting the intuition that highly confident claims should be more expensive to retract.
Under \emph{structure-preserving cost}, deletions are penalized more heavily than additions, $\kappa(\mathsf{del\_\cdot})\;{=}\;w\cdot\kappa(\mathsf{add\_\cdot})$ for some $w>1$, encouraging the solver to repair by augmentation rather than removal.

The repair problem is formalized in Definition~\ref{def:repair}. Given the cost function~$\kappa$ and evidence update~$\Delta$, the solver seeks an optimal repair---a sequence of edit operations of minimum total cost such that $a_t$ attains the desired status under~$\sigma$.

\begin{example}[Continuing Example~\ref{ex:running}]\label{ex:cost}
Under confidence-weighted cost with $\gamma_5 = 0.90$ (a verified lab result) and $\gamma_3 = 0.75$ (a symptomatic inference), deleting~$a_5$ costs $\kappa(\mathsf{del\_arg}(a_5)) = 0.90$.
The augmentation repair $\langle \mathsf{add\_arg}(a_6), \mathsf{add\_att}(a_6, a_5) \rangle$ avoids removing any high-confidence argument, yielding total cost $2\kappa(\mathsf{add\_\cdot})$; this repair is cheaper whenever $\kappa(\mathsf{add\_\cdot}) < 0.45$.
Under structure-preserving cost with $w=2$, deleting~$a_5$ costs~$2$ while the augmentation still costs~$2$, making the two equally expensive and allowing domain preferences to break the tie.
\end{example}

\textbf{ASP Encoding.}
We encode the repair problem as an answer set program following the methodology of Egly et al.~\cite{egly2010asparg} for argumentation reasoning and extending it with choice rules for repair operations.
The encoding consists of three components; at a high level, it mirrors an integer linear program where binary variables select edits, constraints enforce semantics, and the objective minimizes cost.
First, \emph{generate rules} introduce choice atoms for each candidate operation: the solver may optionally add or delete any argument or attack within a bounded edit budget.
Second, \emph{semantics constraints} enforce that the repaired framework satisfies~$\sigma$; for grounded semantics, these take the form of integrity constraints requiring that every argument in the grounded extension defends itself against all attackers.
Third, a \emph{weak constraint} minimizes the weighted sum of selected operations:
\[
  \mathsf{\#minimize}\bigl\{\kappa(o) : \mathsf{selected}(o)\bigr\}.
\]
Continuing with Example~\ref{ex:running}, the choice atoms include $\mathsf{add\_arg}(a_6)$ and $\mathsf{add\_att}(a_6, a_5)$, and the integrity constraints verify that $a_4$ belongs to the grounded extension of the repaired framework.
Algorithm~\ref{alg:repair} summarizes the complete procedure.
When the solver selects $\mathsf{add\_arg}(a)$, the natural-language claim for the new argument is generated by prompting the LLM to produce a rebuttal of the target's attacker, conditioned on the evidence update~$\Delta$; the resulting candidate is verified through the same NLI pipeline before admission.

\begin{algorithm}[tb]
\caption{\textsc{Argus} Repair}\label{alg:repair}
\begin{algorithmic}[1]
\REQUIRE $\mathit{AF}=(\mathcal{A},\mathcal{R})$, semantics $\sigma$, target $a_t$, desired status $s$, evidence $\Delta$, cost function $\kappa$, neighborhood bound $k$
\ENSURE Optimal repair $\mathit{Ops}^*$
\STATE $\mathcal{A}_{\Delta},\mathcal{R}_{\Delta} \leftarrow \textsc{Incorporate}(\mathit{AF}, \Delta)$
\STATE $\mathcal{N} \leftarrow k\text{-neighborhood of } a_t \text{ in } (\mathcal{A}\cup\mathcal{A}_{\Delta},\;\mathcal{R}\cup\mathcal{R}_{\Delta})$
\STATE $\Pi \leftarrow \textsc{EncodeASP}(\mathcal{N}, \sigma, a_t, s, \kappa)$
\STATE $M^* \leftarrow \textsc{Solve}(\Pi)$ \COMMENT{optimal answer set}
\STATE $\mathit{Ops}^* \leftarrow \{o \mid \mathsf{selected}(o) \in M^*\}$
\RETURN $\mathit{Ops}^*$
\end{algorithmic}
\end{algorithm}

\textbf{Approximation for Scalability.}
Even under preferred semantics the repair problem is NP-complete (Theorem~\ref{thm:complexity}), rising to $\Sigma_2^P$-completeness under skeptical stable semantics~\cite{dvorak2012computational}, so we introduce two approximation strategies.
First, a $k$-neighborhood restriction limits the search space to arguments within undirected distance~$k$ of the target in the attack graph; in our experiments, setting $k{=}3$ recovered optimal repairs in 99.7\% of cases while substantially reducing solver grounding.
Second, when ASP solvers are unavailable, beam search over repair sequences with width~$b$ provides a bounded-depth heuristic alternative.
The approximation can miss optimal repairs when the only viable defender lies at distance greater than~$k$ from the target---a scenario that requires long attack chains.  In the LLM explanation frameworks we study, argument graphs are shallow (median depth~3, maximum~7), so $k{=}3$ is sufficient; for deeper domains, $k$ should be increased accordingly.
The $k$-neighborhood restriction is used in all our experiments and ensures scalability to frameworks with hundreds of arguments without sacrificing repair quality.
